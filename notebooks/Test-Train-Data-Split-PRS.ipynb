{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Rahul Kumar. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-Train Data Split using PRS\n",
    "This notebook demonstrates how to carry out the test train split for larget dataset using ``ParallelRunStep`` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "dstore = ws.get_default_datastore()\n",
    "\n",
    "experiment = Experiment(ws, \"train_test_split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "# Name your cluster\n",
    "compute_name = \"general-purpose\"\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(\"Found compute target: \" + compute_name)\n",
    "else:\n",
    "    print(\"Creating a new compute target...\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(\n",
    "        vm_size=\"STANDARD_DS3_V2\", max_nodes=10\n",
    "    )\n",
    "    # Create the compute target\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # If no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=20\n",
    "    )\n",
    "\n",
    "    # For a more detailed view of current cluster status, use the 'status' property\n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import Environment\n",
    "\n",
    "aml_run_config = RunConfiguration()\n",
    "aml_run_config.target = compute_target\n",
    "\n",
    "USE_CURATED_ENV = True\n",
    "if USE_CURATED_ENV:\n",
    "    curated_environment = Environment.get(\n",
    "        workspace=ws, name=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\"\n",
    "    )\n",
    "    aml_run_config.environment = curated_environment\n",
    "else:\n",
    "    aml_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "    # Add some packages relied on by data prep step\n",
    "    aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "        conda_packages=[\"pandas\", \"scikit-learn\"],\n",
    "        pip_packages=[\n",
    "            \"azureml-sdk\",\n",
    "            \"azureml-core\",\n",
    "            \"azureml-dataset-runtime[fuse,pandas]\",\n",
    "        ],\n",
    "        pin_sdk_version=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "- The original dataset is present in the default datastore of the workspace under ``my_files/original/``.\n",
    "- The data after splitting will be stored in the default datastore of the workspace under ``my_files/split/``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore, Dataset\n",
    "from azureml.data.output_dataset_config import OutputFileDatasetConfig\n",
    "\n",
    "input_ds = Dataset.File.from_files(\n",
    "    path=dstore.path(\"my_files/original/*\"), validate=False\n",
    ")\n",
    "\n",
    "output_dir = OutputFileDatasetConfig(\n",
    "    name=\"split\", destination=(dstore, \"my_files/split\")\n",
    ").as_upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure PRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory=\"../scripts\",\n",
    "    entry_script=\"test_train_split_prs.py\",\n",
    "    mini_batch_size=\"1\",\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=aml_run_config.environment,\n",
    "    compute_target=compute_target,\n",
    "    node_count=2,\n",
    "    process_count_per_node=1,\n",
    ")\n",
    "\n",
    "parallel_run_step = ParallelRunStep(\n",
    "    name=\"test train split prs\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[input_ds.as_named_input(\"input_ds\")],\n",
    "    arguments=[\"--train_folder\", \"train1\", \"--test_folder\", \"test1\"],\n",
    "    output=output_dir,\n",
    "    allow_reuse=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure and Submit the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=parallel_run_step)\n",
    "run = experiment.submit(pipeline)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6db9c8d9f0cce2d9127e384e15560d42c3b661994c9f717d0553d1d8985ab1ea"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
