{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Rahul Kumar. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test-Train Data Split using parallel_run_function\n",
    "This notebook demonstrates how to carry out the test train split for larger dataset using ``parallel_run_function``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "ml_client = MLClient.from_config(\n",
    "    DefaultAzureCredential()\n",
    ")\n",
    "\n",
    "ws = ml_client.workspaces.get(name=ml_client.workspace_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.exceptions import ResourceNotFoundError\n",
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "cluster_name = \"gen-purpose\"\n",
    "\n",
    "try:\n",
    "    # Retrieve an already attached Azure Machine Learning Compute.\n",
    "    compute = ml_client.compute.get(cluster_name)\n",
    "except ResourceNotFoundError as e:\n",
    "    compute = AmlCompute(\n",
    "        name=cluster_name,\n",
    "        size=\"STANDARD_D16S_V3\",\n",
    "        type=\"amlcompute\",\n",
    "        min_instances=0,\n",
    "        max_instances=10,\n",
    "        idle_time_before_scale_down=120,\n",
    "    )\n",
    "    poller = ml_client.begin_create_or_update(compute)\n",
    "    poller.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "import os\n",
    "\n",
    "USE_CURATED_ENV = True\n",
    "\n",
    "if USE_CURATED_ENV:\n",
    "    environment = ml_client.environments.get(\n",
    "        name=\"AzureML-sklearn-0.24-ubuntu18.04-py37-cpu\",\n",
    "        label=\"latest\"\n",
    "    )\n",
    "else:\n",
    "    dependencies_dir = \"../dependencies\"\n",
    "    custom_env_name = \"aml-scikit-learn\"\n",
    "\n",
    "    environment = Environment(\n",
    "        name=custom_env_name,\n",
    "        description=\"Custom environment for demo\",\n",
    "        tags={\"scikit-learn\": \"0.24.2\"},\n",
    "        conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
    "        image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
    "    )\n",
    "    environment = ml_client.environments.create_or_update(environment)\n",
    "\n",
    "    print(\n",
    "        f\"Environment with name {environment.name} is registered to workspace, the environment version is {environment.version}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Prepare the data for input and output.\n",
    "- The original dataset is present in the default datastore of the workspace under ``my_files/original/``.\n",
    "- The data after splitting will be stored in the default datastore of the workspace under ``my_files/split/``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "# Supported paths include:\n",
    "# local: './<path>'\n",
    "# blob:  'https://<account_name>.blob.core.windows.net/<container_name>/<path>'\n",
    "# ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/'\n",
    "# Datastore: 'azureml://datastores/<data_store_name>/paths/<path>'\n",
    "\n",
    "dstore = ml_client.datastores.get_default()\n",
    "input_path = f\"azureml://datastores/{dstore.name}/paths/my_files/original\"\n",
    "output_path_train = f\"azureml://datastores/{dstore.name}/paths/my_files/split/train\"\n",
    "output_path_test = f\"azureml://datastores/{dstore.name}/paths/my_files/split/test\"\n",
    "\n",
    "input_data = Input(\n",
    "    path=input_path,\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"The data to be split\",\n",
    "    mode=InputOutputModes.RO_MOUNT\n",
    ")\n",
    "\n",
    "output_train = Output(\n",
    "    path=output_path_train,\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"The test data after split\",\n",
    "    mode=InputOutputModes.RW_MOUNT\n",
    ")\n",
    "\n",
    "output_test = Output(\n",
    "    path=output_path_test,\n",
    "    type=AssetTypes.URI_FOLDER,\n",
    "    description=\"The test data after split\",\n",
    "    mode=InputOutputModes.RW_MOUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure parallel_run_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.parallel import parallel_run_function, RunFunction\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# parallel job to process file data\n",
    "parallel_task = parallel_run_function(\n",
    "    name=\"test train split prs\",\n",
    "    display_name=\"test train split prs\",\n",
    "    description=\"test train split prs\",\n",
    "    inputs=dict(\n",
    "        my_input_path=Input(\n",
    "            type=AssetTypes.URI_FOLDER,\n",
    "            description=\"The data to be split\",\n",
    "        )\n",
    "    ),\n",
    "    outputs=dict(\n",
    "        my_output_path_train=output_train,\n",
    "        my_output_path_test=output_test,\n",
    "        job_output_path=Output(\n",
    "            type=AssetTypes.URI_FILE,\n",
    "            description=\"The path for default output\",\n",
    "        )\n",
    "    ),\n",
    "    input_data=\"${{inputs.my_input_path}}\",\n",
    "    instance_count=1,\n",
    "    mini_batch_size=\"1\",\n",
    "    error_threshold=10,\n",
    "    mini_batch_error_threshold=1,\n",
    "    max_concurrency_per_instance=5,\n",
    "    compute=compute.name,\n",
    "    is_deterministic=False,\n",
    "    task=RunFunction(\n",
    "        code=\"../scripts\",\n",
    "        entry_script=\"test_train_split_prs.py\",\n",
    "        program_arguments=\"--output_dir_train ${{outputs.my_output_path_train}} --output_dir_test ${{outputs.my_output_path_test}}\",\n",
    "        environment=\"azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:1\",\n",
    "        # environment=environment,\n",
    "        append_row_to=\"${{outputs.job_output_path}}\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.dsl import pipeline\n",
    "\n",
    "@pipeline()\n",
    "def parallel_in_pipeline(data_in):\n",
    "    parallel_split = parallel_task(my_input_path=data_in)\n",
    "\n",
    "# create a pipeline\n",
    "pipeline_job = parallel_in_pipeline(input_data)\n",
    "\n",
    "# set pipeline level compute\n",
    "pipeline_job.settings.default_compute = compute.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"demo-pipeline\"\n",
    ")\n",
    "pipeline_job"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('dpv2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1e956f162afe3c1e928078df60ceb136edefb1dd887e3e6dec60c9c74d3f6cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
